{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f107ceec",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36c867c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    raise RuntimeError(\"Ch∆∞a b·∫≠t GPU!\")\n",
    "\n",
    "!pip install --no-deps \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "!pip install --no-deps unsloth_zoo\n",
    "\n",
    "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "\n",
    "!pip install \"transformers>=4.43.3\"\n",
    "\n",
    "print(\"ƒê√£ c√†i xong th∆∞ vi·ªán!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2bbbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "def generate_human_centric_adaptive_data(num_samples=3000):\n",
    "    data = []\n",
    "\n",
    "    knowledge_map = {\n",
    "        \"Frontend\": {\n",
    "            \"React Hooks\": [\"JS ES6 Functions\", \"Closures\", \"State Management Basics\"],\n",
    "            \"CSS Grid\": [\"CSS Box Model\", \"Flexbox\", \"Responsive Design\"],\n",
    "            \"NextJS Routing\": [\"React Components\", \"Client vs Server Side\", \"HTTP Methods\"]\n",
    "        },\n",
    "        \"Backend\": {\n",
    "            \"JWT Authentication\": [\"HTTP Headers\", \"Encryption Basics\", \"RESTful API\"],\n",
    "            \"TypeORM\": [\"SQL Basics\", \"OOP Classes\", \"Database Relations\"],\n",
    "            \"Microservices\": [\"Docker Basics\", \"Monolith Architecture\", \"API Gateway\"]\n",
    "        },\n",
    "        \"CS Fundamentals\": {\n",
    "            \"Dynamic Programming\": [\"Recursion\", \"Big O Notation\", \"Array Manipulation\"],\n",
    "            \"Graph BFS/DFS\": [\"Queue/Stack Data Structure\", \"Adjacency Matrix\", \"Trees\"],\n",
    "            \"Quick Sort\": [\"Recursion\", \"Divide and Conquer\", \"Time Complexity\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    categories = list(knowledge_map.keys())\n",
    "\n",
    "    role_descriptions = [\n",
    "        \"Ki·∫øn th·ª©c n·ªÅn t·∫£ng b·∫Øt bu·ªôc\",\n",
    "        \"Kh√°i ni·ªám ti√™n quy·∫øt quan tr·ªçng\",\n",
    "        \"C∆° s·ªü l√Ω thuy·∫øt c·ªët l√µi\",\n",
    "        \"K·ªπ nƒÉng b·ªï tr·ª£ c·∫ßn thi·∫øt\",\n",
    "        \"Y√™u c·∫ßu ƒë·∫ßu v√†o b·∫Øt bu·ªôc\"\n",
    "    ]\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        category = random.choice(categories)\n",
    "        target_lesson = random.choice(list(knowledge_map[category].keys()))\n",
    "\n",
    "        prereq_lesson = random.choice(knowledge_map[category][target_lesson])\n",
    "\n",
    "        prereq_role_desc = random.choice(role_descriptions)\n",
    "\n",
    "        target_theta = round(random.uniform(-3.0, 3.0), 2)\n",
    "        prereq_theta = round(random.uniform(-3.0, 3.0), 2)\n",
    "\n",
    "        user_input = f\"\"\"\n",
    "B√ÅO C√ÅO H·ªåC T·∫¨P:\n",
    "- B√†i hi·ªán t·∫°i: {target_lesson}\n",
    "- ƒêi·ªÉm nƒÉng l·ª±c (Theta): {target_theta}\n",
    "\n",
    "T√åNH TR·∫†NG KI·∫æN TH·ª®C N·ªÄN T·∫¢NG:\n",
    "- B√†i '{prereq_lesson}' (Vai tr√≤: {prereq_role_desc}) -> Tr·∫°ng th√°i: {\"H·ªîNG KI·∫æN TH·ª®C\" if prereq_theta < 0 else \"T·ªêT\"} (Theta {prereq_theta})\n",
    "\n",
    "H√£y ƒë∆∞a ra quy·∫øt ƒë·ªãnh ƒëi·ªÅu h∆∞·ªõng.\"\"\"\n",
    "\n",
    "        if prereq_theta < 0:\n",
    "            action = \"REVIEW\"\n",
    "            reasons = [\n",
    "                f\"ƒê·ªÉ h·ªçc t·ªët '{target_lesson}', b·∫°n c·∫ßn n·∫Øm v·ªØng '{prereq_lesson}' tr∆∞·ªõc v√¨ ƒë√¢y l√† {prereq_role_desc.lower()}.\",\n",
    "                f\"H·ªá th·ªëng nh·∫≠n th·∫•y ki·∫øn th·ª©c v·ªÅ '{prereq_lesson}' c·ªßa b·∫°n c√≤n l·ªó h·ªïng. H√£y √¥n t·∫≠p l·∫°i b√†i n√†y ƒë·ªÉ x√¢y d·ª±ng n·ªÅn t·∫£ng v·ªØng ch·∫Øc.\",\n",
    "                f\"B√†i '{target_lesson}' y√™u c·∫ßu k·ªπ nƒÉng v·ªÅ '{prereq_lesson}' (l√† {prereq_role_desc.lower()}), nh∆∞ng b·∫°n ch∆∞a ho√†n th√†nh t·ªët ph·∫ßn n√†y.\",\n",
    "                f\"ƒê·ª´ng v·ªôi h·ªçc '{target_lesson}'. H√£y quay l·∫°i c·ªßng c·ªë '{prereq_lesson}' ƒë·ªÉ tr√°nh b·ªã m·∫•t g·ªëc ki·∫øn th·ª©c.\"\n",
    "            ]\n",
    "            explanation = random.choice(reasons)\n",
    "\n",
    "        elif target_theta < -1.0:\n",
    "            action = \"REMEDIAL\"\n",
    "            reasons = [\n",
    "                f\"C√≥ v·∫ª b√†i '{target_lesson}' ƒëang h∆°i qu√° s·ª©c v·ªõi b·∫°n. Ch√∫ng ta h√£y ch·∫≠m l·∫°i v√† l√†m th√™m b√†i t·∫≠p b·ªï tr·ª£ nh√©.\",\n",
    "                f\"K·∫øt qu·∫£ b√†i t·∫≠p cho th·∫•y b·∫°n ƒëang g·∫∑p kh√≥ khƒÉn v·ªõi c√°c concept trong '{target_lesson}'. C·∫ßn h·ªçc k·ªπ l·∫°i ph·∫ßn l√Ω thuy·∫øt.\",\n",
    "                f\"D√π n·ªÅn t·∫£ng ƒë√£ t·ªët, nh∆∞ng b·∫°n v·∫´n ch∆∞a l√†m ch·ªß ƒë∆∞·ª£c '{target_lesson}'. H√£y d√†nh th√™m th·ªùi gian luy·ªán t·∫≠p b√†i n√†y.\",\n",
    "                f\"H·ªá th·ªëng ƒë·ªÅ xu·∫•t l·ªô tr√¨nh b·ªï tr·ª£ ƒë·ªÉ gi√∫p b·∫°n v∆∞·ª£t qua c√°c tr·ªü ng·∫°i hi·ªán t·∫°i trong b√†i '{target_lesson}'.\"\n",
    "            ]\n",
    "            explanation = random.choice(reasons)\n",
    "\n",
    "        else:\n",
    "            action = \"NEXT\"\n",
    "            reasons = [\n",
    "                f\"Tuy·ªát v·ªùi! B·∫°n ƒë√£ n·∫Øm v·ªØng '{target_lesson}' v√† chu·∫©n b·ªã t·ªët n·ªÅn t·∫£ng. H√£y ti·∫øp t·ª•c sang b√†i m·ªõi.\",\n",
    "                f\"Ki·∫øn th·ª©c n·ªÅn t·∫£ng '{prereq_lesson}' v·ªØng ch·∫Øc ƒë√£ gi√∫p b·∫°n chinh ph·ª•c '{target_lesson}' d·ªÖ d√†ng. ƒê·ªß ƒëi·ªÅu ki·ªán ƒëi ti·∫øp.\",\n",
    "                f\"M·ªçi ch·ªâ s·ªë ƒë·ªÅu t·ªët. B·∫°n ƒë√£ s·∫µn s√†ng cho th·ª≠ th√°ch ti·∫øp theo.\",\n",
    "                f\"Ho√†n th√†nh xu·∫•t s·∫Øc '{target_lesson}'. H·ªá th·ªëng ƒë√£ m·ªü kh√≥a n·ªôi dung n√¢ng cao cho b·∫°n.\"\n",
    "            ]\n",
    "            explanation = random.choice(reasons)\n",
    "\n",
    "        response_json = json.dumps({\n",
    "            \"Action\": action,\n",
    "            \"Description\": explanation\n",
    "        }, ensure_ascii=False)\n",
    "\n",
    "        data.append({\"user\": user_input, \"response\": response_json})\n",
    "\n",
    "    return data\n",
    "\n",
    "raw_data = generate_human_centric_adaptive_data(3000)\n",
    "dataset = Dataset.from_list(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8832ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "# Load Qwen 2.5 1.5B\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"unsloth/Qwen2.5-1.5B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "# C·∫•u h√¨nh LoRA N√ÇNG CAO\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, \n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",], \n",
    "    lora_alpha = 64, # Alpha = r * 2\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")\n",
    "\n",
    "print(\"Model Configured for Reasoning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997bf964",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an AI Adaptive Learning Engine.\n",
    "Analyze the student's theta scores and Knowledge Graph to determine the next step.\n",
    "Output format: JSON.\"\"\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    conversations = []\n",
    "    for user, response in zip(examples[\"user\"], examples[\"response\"]):\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user},\n",
    "            {\"role\": \"assistant\", \"content\": response},\n",
    "        ]\n",
    "        conversations.append(conversation)\n",
    "    return { \"text\" : [tokenizer.apply_chat_template(c, tokenize=False) for c in conversations] }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 8, \n",
    "        warmup_steps = 20,\n",
    "        num_train_epochs = 3,           \n",
    "        learning_rate = 5e-5,           \n",
    "\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 10,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\", \n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"üöÄ B·∫Øt ƒë·∫ßu Training\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2b82e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "msg_content = f\"\"\"INPUT DATA:\n",
    "- Lesson: ƒê·∫°o h√†m (Theta: -2.0)\n",
    "- Prerequisite: Gi·ªõi h·∫°n (Theta: -1.5)\n",
    "- Domain: Mathematics\n",
    "\n",
    "Analyze and return JSON.\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": msg_content}\n",
    "]\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate\n",
    "outputs = model.generate(\n",
    "    input_ids = inputs,\n",
    "    max_new_tokens = 256,\n",
    "    use_cache = True,\n",
    "    temperature = 0.1, \n",
    "    do_sample = False  \n",
    ")\n",
    "\n",
    "# Decode\n",
    "clean_response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print(\"K·∫øt qu·∫£ d·ª± ƒëo√°n:\")\n",
    "print(clean_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd3f702",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"adaptive_lora_qwen\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"ƒê√£ l∆∞u model t·∫°i: {save_path}\")\n",
    "\n",
    "!zip -r adaptive_lora_qwen.zip adaptive_lora_qwen\n",
    "\n",
    "from google.colab import files\n",
    "files.download(\"adaptive_lora_qwen.zip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14241cc4",
   "metadata": {},
   "source": [
    "# implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98695d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. C√†i ƒë·∫∑t th∆∞ vi·ªán AI \n",
    "!pip install --no-deps \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps unsloth_zoo xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "!pip install \"transformers>=4.43.3\"\n",
    "\n",
    "# 2. C√†i ƒë·∫∑t th∆∞ vi·ªán Server & Tunnel\n",
    "!pip install fastapi uvicorn pyngrok nest-asyncio\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0742cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile server.py\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import uvicorn\n",
    "from typing import Dict, Any, Optional, List\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.concurrency import run_in_threadpool\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "adaptive_instance = None\n",
    "\n",
    "class AdaptiveModel:\n",
    "    def __init__(self):\n",
    "        print(\"AdaptiveModel: ƒêang kh·ªüi ƒë·ªông Engine...\")\n",
    "\n",
    "        # C·∫§U H√åNH MODEL\n",
    "        base_model_id = \"unsloth/Qwen2.5-1.5B-Instruct\"\n",
    "        adapter_path = \"/content/drive/MyDrive/edtech/adaptive_lora_qwen\"\n",
    "        if not os.path.exists(adapter_path):\n",
    "             adapter_path = \"adaptive_lora_qwen\"\n",
    "\n",
    "        max_seq_length = 2048\n",
    "        dtype = None\n",
    "        load_in_4bit = True\n",
    "\n",
    "        try:\n",
    "            self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "                model_name=base_model_id,\n",
    "                max_seq_length=max_seq_length,\n",
    "                dtype=dtype,\n",
    "                load_in_4bit=load_in_4bit,\n",
    "            )\n",
    "\n",
    "            if os.path.exists(adapter_path):\n",
    "                print(f\"Loading LoRA adapter: {adapter_path}\")\n",
    "                self.model = PeftModel.from_pretrained(\n",
    "                    self.model,\n",
    "                    adapter_path,\n",
    "                    is_trainable=False,\n",
    "                )\n",
    "            else:\n",
    "                print(f\"C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y '{adapter_path}'. ƒêang ch·∫°y Model g·ªëc (Logic c√≥ th·ªÉ k√©m h∆°n).\")\n",
    "\n",
    "            FastLanguageModel.for_inference(self.model)\n",
    "            print(\"AdaptiveModel: S·∫µn s√†ng!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Critical Load Error: {e}\")\n",
    "            self.model = None\n",
    "\n",
    "    def _build_messages(self, data: Dict[str, Any]) -> list:\n",
    "        context = data.get(\"current_context\", {})\n",
    "        kg = data.get(\"knowledge_graph_subgraph\", [])\n",
    "        profile = data.get(\"user_mastery_profile\", [])\n",
    "\n",
    "        curr_id = str(context.get('target_content_id'))\n",
    "        curr_theta = float(context.get('current_theta', 0))\n",
    "\n",
    "        profile_map = {str(item['content_id']): item for item in profile}\n",
    "        curr_info = profile_map.get(curr_id, {})\n",
    "        curr_title = curr_info.get('title', f\"Lesson {curr_id}\")\n",
    "\n",
    "        curr_desc = \"Kh√¥ng c√≥ m√¥ t·∫£ n·ªôi dung.\"\n",
    "        for e in kg:\n",
    "            if str(e.get('target_id')) == curr_id:\n",
    "                curr_desc = e.get('description_target', '')\n",
    "                if curr_desc: break\n",
    "            elif str(e.get('source_id')) == curr_id:\n",
    "                curr_desc = e.get('description_source', '')\n",
    "                if curr_desc: break\n",
    "\n",
    "        prereq_analysis = []\n",
    "        for e in kg:\n",
    "            if str(e['target_id']) == curr_id and e['type'] == 'PREREQUISITE':\n",
    "                src_id = str(e['source_id'])\n",
    "                src_title = e.get('source', 'Unknown Lesson')\n",
    "\n",
    "                src_content = e.get('description_source', 'Kh√¥ng c√≥ m√¥ t·∫£ chi ti·∫øt.')\n",
    "\n",
    "                p_theta = profile_map.get(src_id, {}).get('theta', 0.0)\n",
    "                status = \"H·ªîNG KI·∫æN TH·ª®C\" if p_theta < 0 else \"N·∫ÆM V·ªÆNG\"\n",
    "\n",
    "                prereq_analysis.append(\n",
    "                    f\"- B√†i: {src_title} (ID: {src_id})\\n\"\n",
    "                    f\"  + N·ªôi dung ki·∫øn th·ª©c: {src_content}\\n\"\n",
    "                    f\"  + Tr·∫°ng th√°i h·ªçc vi√™n: Theta {p_theta} ({status})\"\n",
    "                )\n",
    "\n",
    "        next_analysis = []\n",
    "        for e in kg:\n",
    "            if str(e['source_id']) == curr_id:\n",
    "                tgt_id = str(e['target_id'])\n",
    "                tgt_title = e.get('target', 'Unknown Next Lesson')\n",
    "                tgt_content = e.get('description_target', 'N·ªôi dung b√†i ti·∫øp theo')\n",
    "                next_analysis.append(f\"- {tgt_title} (ID: {tgt_id}): {tgt_content}\")\n",
    "\n",
    "        system_prompt = \"\"\"B·∫°n l√†  m·ªôt gi·∫£ng vi√™n, h√£y ƒë∆∞a ra chi·∫øn l∆∞·ª£c gi√∫p h·ªçc vi√™n hi·ªÉu s√¢u v√† b·ªÅn v·ªØng h∆°n v·ªÅ b√†i h·ªçc\n",
    "CHI·∫æN L∆Ø·ª¢C T∆Ø DUY:\n",
    "- ƒê·ªçc n·ªôi dung b√†i h·ªçc hi·ªán t·∫°i v√† c√°c b√†i li√™n quan ƒë·ªÉ hi·ªÉu ki·∫øn th·ª©c c·ªët l√µi ƒëang ƒë∆∞·ª£c s·ª≠ d·ª•ng.\n",
    "- So s√°nh n·ªôi dung ki·∫øn th·ª©c n·ªÅn t·∫£ng v·ªõi m·ª©c ƒë·ªô hi·ªÉu hi·ªán t·∫°i c·ªßa h·ªçc vi√™n.\n",
    "  N·∫øu h·ªçc vi√™n ch∆∞a n·∫Øm ƒë∆∞·ª£c c√°c kh√°i ni·ªám c·∫ßn thi·∫øt cho b√†i hi·ªán t·∫°i, vi·ªác h·ªçc ti·∫øp s·∫Ω k√©m hi·ªáu qu·∫£ ‚Üí ∆∞u ti√™n √¥n t·∫≠p.\n",
    "- N·∫øu ki·∫øn th·ª©c n·ªÅn ƒë√£ ƒë·ªß nh∆∞ng h·ªçc vi√™n v·∫´n g·∫∑p kh√≥ khƒÉn ·ªü b√†i hi·ªán t·∫°i,\n",
    "  h√£y xem ƒë√¢y l√† v·∫•n ƒë·ªÅ v·ªÅ ƒë·ªô kh√≥ ho·∫∑c kh·∫£ nƒÉng √°p d·ª•ng ‚Üí c·∫ßn c·ªßng c·ªë th√™m tr∆∞·ªõc khi h·ªçc m·ªõi.\n",
    "- Ch·ªâ ƒë·ªÅ xu·∫•t h·ªçc ti·∫øp khi h·ªçc vi√™n th·ªÉ hi·ªán s·ª± s·∫µn s√†ng v·ªÅ m·∫∑t hi·ªÉu bi·∫øt,\n",
    "  d·ª±a tr√™n m·ªëi li√™n h·ªá n·ªôi dung gi·ªØa c√°c b√†i h·ªçc, kh√¥ng ch·ªâ d·ª±a v√†o con s·ªë.\n",
    "\n",
    "OUTPUT FORMAT (JSON Only):\n",
    "{\"Action\": \"REVIEW/REMEDIAL/NEXT\", \"Description\": \"Gi·∫£i th√≠ch d·ª±a tr√™n m·ªëi li√™n h·ªá n·ªôi dung b√†i h·ªçc...\"}\"\"\"\n",
    "\n",
    "        # 5. USER PROMPT\n",
    "        user_prompt = f\"\"\"\n",
    "--- B·ªêI C·∫¢NH B√ÄI ƒêANG H·ªåC ---\n",
    "T√™n b√†i: {curr_title} (ID: {curr_id})\n",
    "M√¥ t·∫£ n·ªôi dung: {curr_desc}\n",
    "K·∫øt qu·∫£ hi·ªán t·∫°i: Theta {curr_theta}\n",
    "\n",
    "--- PH√ÇN T√çCH C√ÅC B√ÄI TI·ªÄN ƒê·ªÄ (PREREQUISITES) ---\n",
    "{chr(10).join(prereq_analysis) if prereq_analysis else \"Kh√¥ng c√≥ b√†i ti·ªÅn ƒë·ªÅ (ƒê√¢y l√† b√†i nh·∫≠p m√¥n).\"}\n",
    "\n",
    "--- C√ÅC L·ª∞A CH·ªåN TI·∫æP THEO ---\n",
    "{chr(10).join(next_analysis) if next_analysis else \"Kh√¥ng c√≥ d·ªØ li·ªáu b√†i ti·∫øp theo.\"}\n",
    "\n",
    "D·ª±a tr√™n n·ªôi dung v√† k·∫øt qu·∫£ tr√™n, h√£y ƒë∆∞a ra quy·∫øt ƒë·ªãnh.\"\"\"\n",
    "\n",
    "        return [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}]\n",
    "    def _resolve_target_id(self, action: str, data: Dict[str, Any]) -> str:\n",
    "        kg = data.get(\"knowledge_graph_subgraph\", [])\n",
    "        profile = data.get(\"user_mastery_profile\", [])\n",
    "        curr_id = str(data[\"current_context\"]['target_content_id'])\n",
    "\n",
    "        # Map Theta\n",
    "        p_map = {str(item['content_id']): float(item['theta']) for item in profile}\n",
    "\n",
    "        # Case 1: REVIEW\n",
    "        if action == \"REVIEW\":\n",
    "            candidates = []\n",
    "            for e in kg:\n",
    "                if str(e['target_id']) == curr_id and e['type'] == 'PREREQUISITE':\n",
    "                    src_id = str(e['source_id'])\n",
    "                    theta = p_map.get(src_id, 0.0)\n",
    "                    if theta < 0:\n",
    "                        candidates.append((src_id, theta))\n",
    "\n",
    "            if candidates:\n",
    "                candidates.sort(key=lambda x: x[1])\n",
    "                return candidates[0][0]\n",
    "\n",
    "            return curr_id\n",
    "\n",
    "        # Case 2: NEXT\n",
    "        if action == \"NEXT\":\n",
    "            for e in kg:\n",
    "                if str(e['source_id']) == curr_id and e['type'] == \"RELATED\":\n",
    "                    return str(e['target_id'])\n",
    "\n",
    "        # Case 3: REMEDIAL\n",
    "        return curr_id\n",
    "\n",
    "    def predict(self, request_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        if self.model is None:\n",
    "            return self._fallback_response(request_data, \"Model not loaded\")\n",
    "\n",
    "        request_id = request_data.get(\"request_id\")\n",
    "\n",
    "        try:\n",
    "            messages = self._build_messages(request_data)\n",
    "\n",
    "            device = self.model.device\n",
    "            inputs = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=True,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=inputs,\n",
    "                    max_new_tokens=256,\n",
    "                    do_sample=False,\n",
    "                    temperature=0.0\n",
    "                )\n",
    "\n",
    "            response_text = self.tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "            # Extract JSON\n",
    "            match = re.search(r'\\{[\\s\\S]*\\}', response_text)\n",
    "            if match:\n",
    "                ai_result = json.loads(match.group(0))\n",
    "                action = ai_result.get(\"Action\", \"NEXT\").upper()\n",
    "                description = ai_result.get(\"Description\", \"AI Recommendation\")\n",
    "\n",
    "                final_target_id = self._resolve_target_id(action, request_data)\n",
    "\n",
    "                if action == \"REVIEW\" and final_target_id == str(request_data[\"current_context\"]['target_content_id']):\n",
    "                    action = \"REMEDIAL\"\n",
    "                    description = \"ƒê·ªÅ xu·∫•t √¥n t·∫≠p b√†i hi·ªán t·∫°i.\"\n",
    "\n",
    "                return {\n",
    "                    \"request_id\": request_id,\n",
    "                    \"suggestion\": {\n",
    "                        \"Action\": action,\n",
    "                        \"TargetID\": final_target_id,\n",
    "                        \"Description\": description\n",
    "                    },\n",
    "                    \"status\": \"SUCCESS\"\n",
    "                }\n",
    "            else:\n",
    "                raise Exception(\"Invalid JSON Format\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Fallback triggered: {e}\")\n",
    "            return self._fallback_response(request_data, str(e))\n",
    "\n",
    "    def _fallback_response(self, data, error_msg):\n",
    "        fallback = self._fallback_logic(data)\n",
    "        return {\n",
    "            \"request_id\": data.get(\"request_id\"),\n",
    "            \"suggestion\": fallback,\n",
    "            \"status\": \"SUCCESS\",\n",
    "            \"debug_info\": error_msg\n",
    "        }\n",
    "\n",
    "    def _fallback_logic(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "\n",
    "      ctx = data.get(\"current_context\", {})\n",
    "      kg = data.get(\"knowledge_graph_subgraph\", [])\n",
    "      profile = data.get(\"user_mastery_profile\", [])\n",
    "\n",
    "      curr_id = str(ctx.get('target_content_id'))\n",
    "      curr_theta = float(ctx.get('current_theta', 0))\n",
    "\n",
    "      p_map = {str(i['content_id']): float(i['theta']) for i in profile}\n",
    "\n",
    "      prereq_candidates = []\n",
    "      for e in kg:\n",
    "          if str(e.get('target_id')) == curr_id and e.get('type') == 'PREREQUISITE':\n",
    "              prereq_id = str(e.get('source_id'))\n",
    "              theta = p_map.get(prereq_id, 0.0)\n",
    "              if theta < 0:\n",
    "                  prereq_candidates.append((prereq_id, theta, e))\n",
    "\n",
    "      if prereq_candidates:\n",
    "          prereq_candidates.sort(key=lambda x: x[1])\n",
    "          prereq_id, theta, edge = prereq_candidates[0]\n",
    "\n",
    "          return {\n",
    "              \"Action\": \"REVIEW\",\n",
    "              \"TargetID\": prereq_id,\n",
    "              \"Description\": f\"Ph√°t hi·ªán h·ªïng ki·∫øn th·ª©c n·ªÅn t·∫£ng '{edge.get('source', '')}'\"\n",
    "          }\n",
    "\n",
    "      if curr_theta < -1.0:\n",
    "          for e in kg:\n",
    "              if str(e.get('source_id')) == curr_id and e.get('type') == 'REMEDIAL':\n",
    "                  return {\n",
    "                      \"Action\": \"REMEDIAL\",\n",
    "                      \"TargetID\": str(e.get('target_id')),\n",
    "                      \"Description\": \"ƒê·ªÅ xu·∫•t √¥n t·∫≠p b√†i remedial do nƒÉng l·ª±c hi·ªán t·∫°i th·∫•p\"\n",
    "                  }\n",
    "\n",
    "          return {\n",
    "              \"Action\": \"REMEDIAL\",\n",
    "              \"TargetID\": curr_id,\n",
    "              \"Description\": \"ƒê·ªÅ xu·∫•t √¥n t·∫≠p l·∫°i b√†i hi·ªán t·∫°i do nƒÉng l·ª±c c√≤n y·∫øu\"\n",
    "          }\n",
    "\n",
    "      for e in kg:\n",
    "          if str(e.get('source_id')) == curr_id and e.get('type') == 'RELATED':\n",
    "              return {\n",
    "                  \"Action\": \"NEXT\",\n",
    "                  \"TargetID\": str(e.get('target_id')),\n",
    "                  \"Description\": \"ƒê·ªß ƒëi·ªÅu ki·ªán chuy·ªÉn sang b√†i h·ªçc ti·∫øp theo\"\n",
    "              }\n",
    "\n",
    "      return {\n",
    "          \"Action\": \"NEXT\",\n",
    "          \"TargetID\": curr_id,\n",
    "          \"Description\": \"Kh√¥ng t√¨m th·∫•y l·ª±a ch·ªçn ph√π h·ª£p h∆°n, gi·ªØ nguy√™n b√†i hi·ªán t·∫°i\"\n",
    "      }\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    global adaptive_instance\n",
    "    adaptive_instance = AdaptiveModel()\n",
    "    yield\n",
    "    print(\"Shutting down...\")\n",
    "\n",
    "app = FastAPI(lifespan=lifespan)\n",
    "\n",
    "class CurrentContext(BaseModel):\n",
    "    target_content_id: str\n",
    "    current_theta: float\n",
    "\n",
    "class KnowledgeGraphEdge(BaseModel):\n",
    "    source_id: str\n",
    "    source: str\n",
    "    description_source: Optional[str] = \"\"\n",
    "    target_id: str\n",
    "    target: str\n",
    "    type: str\n",
    "    description_target: Optional[str] = \"\"\n",
    "\n",
    "class UserMastery(BaseModel):\n",
    "    content_id: str\n",
    "    title: Optional[str] = None\n",
    "    theta: float\n",
    "    status: str\n",
    "\n",
    "class AdaptiveRequest(BaseModel):\n",
    "    request_id: str\n",
    "    current_context: CurrentContext\n",
    "    knowledge_graph_subgraph: List[KnowledgeGraphEdge]\n",
    "    user_mastery_profile: List[UserMastery]\n",
    "\n",
    "@app.post(\"/adaptive\")\n",
    "async def navigate_learning(request: AdaptiveRequest):\n",
    "    input_data = request.model_dump()\n",
    "\n",
    "    result = await run_in_threadpool(adaptive_instance.predict, input_data)\n",
    "    return result\n",
    "\n",
    "@app.get(\"/\")\n",
    "def home():\n",
    "    return {\"message\": \"AI Server is Online\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855c5a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from pyngrok import ngrok\n",
    "import time\n",
    "NGROK_TOKEN = \"373glv84UEC0goVEx7XtaTAYeDd_4LkpbJTdct429XbP8QmKG\"\n",
    "ngrok.set_auth_token(NGROK_TOKEN)\n",
    "\n",
    "ngrok.kill()\n",
    "\n",
    "public_url = ngrok.connect(8000).public_url\n",
    "print(f\"üöÄ SERVER ƒêANG CH·∫†Y T·∫†I PUBLIC URL: {public_url}\")\n",
    "print(f\"üëâ API Endpoint cho NestJS: {public_url}/adaptive\")\n",
    "print(\"‚è≥ ƒêang kh·ªüi ƒë·ªông Model (M·∫•t kho·∫£ng 1-2 ph√∫t)...\")\n",
    "!uvicorn server:app --host 0.0.0.0 --port 8000"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
